{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem.snowball import ItalianStemmer\n",
    "import nltk\n",
    "from num2words import num2words\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "\n",
    "The **TripAdvisor dataset** chosen for this project contains information regarding **italian reviews on the famous travel platform**.\n",
    "\n",
    "The dataset is composed of **28754 labelled textual reviews** and it has **no missing values / inconsistencies**.\n",
    "\n",
    "Its attributes are:<br/>\n",
    "{<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"**text**\": string - The review<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"**class**\": boolean (\"pos\" / \"neg\") - The label associated to the review <br/>\n",
    "}<br/>\n",
    "\n",
    "Analyzing the dataset, the first thing which stands out is that **it is label-imbalanced**, considering its very small size and that **almost the 70% of the reviews are classified as positive**.<br/>\n",
    "\n",
    "However, **the negative ones are tendentially longer**: in fact, the average length for negative reviews is around 140 words, in contrast to the average 100 words of the positive ones.<br/>\n",
    "\n",
    "Due to this fact, we can assume that **there is the potential to extract more features from an average negative review than from an average positive review**, trying to reduce the gap setted by the imbalance of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development dataset:\n",
      "\n",
      "Shape: (28754, 2)\n",
      "\n",
      "Values counts:\n",
      "pos    19532\n",
      "neg     9222\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "Evaluation dataset:\n",
      "\n",
      "Shape: (12323, 1)\n"
     ]
    }
   ],
   "source": [
    "dev = pd.read_csv('exam_development.csv')\n",
    "eval = pd.read_csv('exam_evaluation.csv')\n",
    "\n",
    "print(\"Development dataset:\\n\")\n",
    "print(\"Shape: \" + str(dev.shape) + \"\\n\")\n",
    "print(\"Values counts:\\n\" + str(dev.loc[:, \"class\"].value_counts()) + \"\\n\\n\\n\")\n",
    "\n",
    "print(\"Evaluation dataset:\\n\")\n",
    "print(\"Shape: \" + str(eval.shape))\n",
    "\n",
    "dict = dev.loc[:, \"class\"].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The **preprocessing pipeline** that I have scheduled for this project is the following: \n",
    "1. Conversion to lower case\n",
    "2. Punctuation removal\n",
    "3. Italian stopwords removal\n",
    "4. Stemming process\n",
    "\n",
    "First of all, I decided to transform every review to **lower case**, in order to help the other steps (in particular the stopwords removal and stemming processes) of the pipeline.\n",
    "\n",
    "I also replaced every \" ' \" with a space, in order to properly separate the words.\n",
    "\n",
    "Then, I removed all the punctuation from the reviews, including \"!\".<br/>\n",
    "Initially I did not want to take it off, because it could be useful to **increase the strength of a sentence** and **enlarge its polarity** (for example from \"quite positive\" to \"very positive\"), but considering that we are in a \"boolean\" case, I decided to remove it.\n",
    "\n",
    "Regarding the two last steps, which work directly on words, I tried different approaches.<br/> \n",
    "At first I created a bag of words from the union of three sets:\n",
    "- Stop words from the Python library stop_words â€¢ Stop words from the Python library NLTK\n",
    "- First 100 numbers\n",
    "\n",
    "After some time I figured out that **having numbers inside my bag of words is a potentially dangerous approach in terms of losing features**, so I ended up that the best set was the one from stop_words library in union with the one from NLTK.\n",
    "\n",
    "I also removed the word \"non\" from the set, because I think that it is extremely important in the context of the sentences.\n",
    "\n",
    "The stemmer I chose is ItalianStemmer from NLTK.<br/>\n",
    "I used a stemmer in order to truncate superlatives, avoid taking care of the gender of nouns and normalize the vocabulary.\n",
    "\n",
    "After this preprocessing pipeline, I used TfidfVectorizer from sklearn in order to transform the reviews into a TF-IDF features matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development dataset\n",
      "-  -  -  -  -  -  -  -  -\n",
      "Lower case text\n",
      "Remove punctuation\n",
      "Remove italian stopwords\n",
      "Stemming process\n",
      "- - - - - - - - - - - -\n",
      "Evaluation dataset\n",
      "-  -  -  -  -  -  -  -  -\n",
      "Lower case text\n",
      "Remove punctuation\n",
      "Remove italian stopwords\n",
      "Stemming process\n"
     ]
    }
   ],
   "source": [
    "# Stemmer\n",
    "stemmer = ItalianStemmer()\n",
    "\n",
    "print(\"Development dataset\")\n",
    "print(\"-  -  -  -  -  -  -  -  -\")\n",
    "print(\"Lower case text\")\n",
    "dev.loc[:, \"text\"] = dev.text.apply(lambda x: str.lower(x).replace(\"'\", \" \"))\n",
    "\n",
    "print(\"Remove punctuation\")\n",
    "dev.loc[:, \"text\"] = dev.text.apply(lambda x: \" \".join(re.findall('[\\w]+', x)))\n",
    "\n",
    "print(\"Remove italian stopwords\")\n",
    "dev.loc[:, \"text\"] = dev.text.apply(lambda x: removeStopWords(x))\n",
    "\n",
    "print(\"Stemming process\")\n",
    "dev.loc[:, \"text\"] = dev.text.apply(lambda x: stem(x))\n",
    "\n",
    "print(\"- - - - - - - - - - - -\")\n",
    "\n",
    "print(\"Evaluation dataset\")\n",
    "print(\"-  -  -  -  -  -  -  -  -\")\n",
    "\n",
    "print(\"Lower case text\")\n",
    "eval.loc[:, \"text\"] = eval.text.apply(lambda x: str.lower(x).replace(\"'\", \" \"))\n",
    "\n",
    "print(\"Remove punctuation\")\n",
    "eval.loc[:, \"text\"] = eval.text.apply(lambda x: \" \".join(re.findall('[\\w]+', x)))\n",
    "\n",
    "print(\"Remove italian stopwords\")\n",
    "eval.loc[:, \"text\"] = eval.text.apply(lambda x: removeStopWords(x))\n",
    "\n",
    "print(\"Stemming process\")\n",
    "eval.loc[:, \"text\"] = eval.text.apply(lambda x: stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm choice\n",
    "\n",
    "#### Random Forest\n",
    "As a first approach, I used the **RandomForest** from Python's sklearn package as a base classifier. The test set was made up of the 20% of the development set.\n",
    "\n",
    "Before doing any types of tuning, the classifier reached 91% of accuracy, which was not absolutely bad.\n",
    "\n",
    "After this first attempt, I did some **tuning to the hyperparameters** (that are better explained in the next section) that lead the classifier to 93% of accuracy.\n",
    "\n",
    "#### Logistic Regressor\n",
    "\n",
    "In addition, I tried to use the **LogisticRegressor**, still from the sklearn package; I thought that this regressor could be a good one for this project because of its capability on **modelling binary classification problems**.\n",
    "\n",
    "Using the LogisticRegressor the accuracy reached 95%.\n",
    "\n",
    "Finally, I decided to try the **LinearSVC classifier** from sklearn, assuming that with the preprocessing steps done before and some tuning on the TfidfVectorizer the dataset was splitted clearly into two different clusters.\n",
    "\n",
    "In conclusion, after a little bit of tuning, the LinearSVC reached **97.5% of accuracy**.\n",
    "\n",
    "#### Linear Support Vector Machine\n",
    "\n",
    "Even if Support Vector Machine algorithm (with linear kernel) perform similarly to the Logistic Regression, I think that the first one performs better on this project because of the **sensitivity to marginal values**. \n",
    "\n",
    "The sigmoid function of the LogisticRegressor tends to not properly identify simil-neutral values, while the Support Vector Machine algorithm tries to construct the best widest possible separating line to split this two clusters.\n",
    "\n",
    "All the evaluation for the accuracy were done with f1_score, as suggested in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning and validation\n",
    "\n",
    "In order to tune properly the classifiers I used the **GridSearchCV** to tune one main parameter, C. \n",
    "I thought that this was the most important parameter I should have worked on because of the problems with marginal values that I had.\n",
    "\n",
    "After some attempts, I ended up that **C = 5 was the best in terms of accuracy**. So, my classifier work best with a small margin around the hyperplane of the LinearSVC.\n",
    "\n",
    "\"**class_weight**\" is another crucial parameter I setted. <br/>\n",
    "In fact, we still have to keep in mind that the dataset was imbalanced, so I had to give to the classifier a way to rebalance this disparity.\n",
    "\n",
    "For the **TfidfVectorizer**, I setted *max_df* to 0.3 to prune eventual corpus-specific stop words. I think that this parameter could be setted also up to 0.4.\n",
    "\n",
    "Moreover, I fixed *ngram_range* to (1, 2), especially because I decided to left the \"non\" word outside of the stopwords and so inside of the dataset. <br/>\n",
    "I tried also with trigrams, considering that in the italian language the word \"non\" could appear quite far to the main concept of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit and Predict:\n",
      "Accuracy for C:0.5 \n",
      "(accuracy_score):0.9680055642496957\n",
      "(f1_score): 0.9679298239468118\n",
      "Accuracy for C:1 \n",
      "(accuracy_score):0.9680055642496957\n",
      "(f1_score): 0.9679298239468118\n",
      "Accuracy for C:5 \n",
      "(accuracy_score):0.9680055642496957\n",
      "(f1_score): 0.9679298239468118\n",
      "Accuracy for C:10 \n",
      "(accuracy_score):0.9680055642496957\n",
      "(f1_score): 0.9679298239468118\n",
      "\n",
      "\n",
      "Best positive words:\n",
      "('perfett', 4.315061287108056)\n",
      "('eccellent', 4.122722642589093)\n",
      "('fantast', 3.511618121202611)\n",
      "('confortevol', 3.191626867207938)\n",
      "('po', 3.187816105687944)\n",
      "\n",
      "Best negative words:\n",
      "('pessim', -4.904545308063406)\n",
      "('sporc', -4.551094328473539)\n",
      "('scars', -3.9961653985313177)\n",
      "('scortes', -3.7267099986243837)\n",
      "('vecc', -3.3512247498507683)\n"
     ]
    }
   ],
   "source": [
    "# Local training\n",
    "cv = TfidfVectorizer(ngram_range=(1, 2), binary=True, max_df=0.3)\n",
    "cv.fit(dev.loc[:, \"text\"])\n",
    "X = cv.transform(dev.loc[:, \"text\"])\n",
    "\n",
    "print(\"Fit and Predict:\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, dev.loc[:, \"class\"], test_size=0.2, random_state=0)\n",
    "\n",
    "for c in [0.5, 1, 5, 10]:\n",
    "    lr = svm.LinearSVC(class_weight=dict, C=c, max_iter=15000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    predictions = lr.predict(X_test)\n",
    "    print(\"Accuracy for C:%s \\n(accuracy_score):%s\"\n",
    "          % (c, accuracy_score(y_test, predictions)))\n",
    "    print(\"(f1_score):\", f1_score(y_test, predictions, average='weighted'))\n",
    "\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), lr.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"\\n\\nBest positive words:\")\n",
    "for best_positive in sorted(\n",
    "        feature_to_coef.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True)[:5]:\n",
    "    print(best_positive)\n",
    "print(\"\\nBest negative words:\")\n",
    "for best_negative in sorted(\n",
    "        feature_to_coef.items(),\n",
    "        key=lambda x: x[1])[:5]:\n",
    "    print(best_negative)\n",
    "\n",
    "cv = TfidfVectorizer(ngram_range=(1, 2), max_df=0.3)\n",
    "cv.fit(dev.loc[:, \"text\"])\n",
    "X = cv.transform(dev.loc[:, \"text\"])\n",
    "X_test = cv.transform(eval.loc[:, \"text\"])\n",
    "\n",
    "lr = svm.LinearSVC(class_weight=dict, max_iter=15000)\n",
    "lr.fit(X, dev.loc[:, \"class\"])\n",
    "\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "with open('exam_export.csv', 'w') as file:\n",
    "    file.write(\"Id,Predicted\\n\")\n",
    "    for index in eval.index:\n",
    "        s = predictions[index]\n",
    "        file.write(str(index) + \",\" + s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('italian')\n",
    "\n",
    "def removeStopWords(s):\n",
    "    s = ' '.join(word for word in s.split() if word not in stopWords)\n",
    "    return s\n",
    "\n",
    "def stem(s):\n",
    "    global stemmer\n",
    "    s = ' '.join(stemmer.stem(word) for word in s.split())\n",
    "    return s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
